{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Honors Project Report: Distrubuted Facial Expression Recognition\n",
    "#### Samuel Zeleke & Enoch Mwesigwa\n",
    "\n",
    "\n",
    "### Vision\n",
    "Multi-label classification and object detection models have gained significant popularity in recent years. They've become\n",
    "integral components of systems ranging from autonomous cars and security systems, to social media platforms and search\n",
    "engines. Our project is aimed at creating a system that correctly labels facial expressions in real-time using an Aryzon headset and\n",
    "CNNs built on keras. We'll use OpenCV's cascade classifier to extract faces from frames, and train a separate CNN classify the facial\n",
    "expression. The output is used to draw a labelled box around faces on display the results on the client.\n",
    "\n",
    "### Background\n",
    "Object detection entails recognizing multiple objects places at different locations in the image. Unfortunately, regular\n",
    "covNets cannot (at least not \"normally\") cannot solve this problem: their architecture only allows inputs and outputs\n",
    "with fixed sizes. So, they are restricted to merely labelling images.\n",
    "\n",
    "There have been several attempts to go around this problem. The most significant ones are using R-CNN and YOLO. [R-CNN](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)\n",
    "and its decedents use additional preprocessing to generate thousands of candidate regions (called \"proposals\") in the images\n",
    "and pass each region to a covNet for classification. Obviously, this is a very resource-intensive process, (training in\n",
    "some architectures takes days) and they are not fast enough for real-time object recognition. [YOLO](https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193), on the other hand, uses a single (very) deep CovNet to\n",
    "both recognize regions of interest and to classify those regions. This method is several times faster and more\n",
    "efficient than R-CNNs. Unfortunately, the architecture needs a lot of data for training and uses NN layers we were not familiar with.\n",
    "\n",
    "So, building on Gurav Sharma's article \"[Real Time Facial Recognition](https:/medium.com/datadriveninvestor/real-time-facial-expression-recognition-f860dacfeb6a)\",\n",
    "we chose to create a simpler system that combines openCV's trained cascade classifiers to extract the\n",
    "faces and trained a small NN to classify the facial expressions. This largely avoids R-CNNs inefficiencies and significantly reduces the\n",
    "size of training data we need to get decent predictions.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Training\n",
    "Like R-CNNs, our system divides the facial expression task into two stages. In the first stage, we use openCV's pre-trained cascade classifiers\n",
    "to find regions containing faces. For the second stage, Sharma recommends taking advantage of the Keras' pretrained models using transfer learning.\n",
    "(Transfer learning involves using layers from a model trained for a different dataset. It let's the recepient model to take advantage of the \"donor\"\n",
    "model's training by using its weights for predictions.) Unfortunately, we didn't have enough training data to achieve significant training set accuracy.\n",
    "Additionally, the resulting models were taking a lot of storage. So, instead we adopted the architecture we used for the fashion mnist homework to build a small\n",
    "CNN that classified facial expressions.\n",
    "\n",
    "*Code for second-stage model training*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(batch_size = 10):\n",
    "    import os\n",
    "    import zipfile\n",
    "    import cv2\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import layers\n",
    "    # import matplotlib.pyplot as plt\n",
    "    import numpy\n",
    "\n",
    "    # unzipfile\n",
    "    with zipfile.ZipFile(\"/content/drive/My Drive/tif_extended.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./trainingSrc\")\n",
    "\n",
    "    PATH = \"./trainingSrc/tif_extended\"\n",
    "    CLASSES = os.listdir(PATH)\n",
    "\n",
    "    image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split = 0.2)\n",
    "    images = image_generator.flow_from_directory(\n",
    "        PATH,\n",
    "        batch_size = 1,\n",
    "        target_size = (256, 256),\n",
    "    )\n",
    "    sample, label = next(images)\n",
    "    feature_dataframe = []\n",
    "    target_dataframe = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "      feature_dataframe.append(images[i][0][0])\n",
    "      target_dataframe.append(images[i][1][0])\n",
    "\n",
    "    feature_dataframe = numpy.array(feature_dataframe)\n",
    "    target_dataframe = numpy.array(target_dataframe)\n",
    "\n",
    "    # create network\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # input and first convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(30, 2, activation=\"relu\", input_shape = (256, 256, 3)))\n",
    "    model.add(keras.layers.Conv2D(60, kernel_size=5, strides=(2, 2), activation=\"relu\"))#(60, 5, stri activation=\"relu\"))\n",
    "    # input and second convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(30, 3, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    # input and third convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    #flatten\n",
    "    model.add(keras.layers.Flatten())\n",
    "    # three dense layers\n",
    "    model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(28, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(7, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "    x = feature_dataframe,\n",
    "    y = target_dataframe,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding useful training data for our project was a challenging process. Most face detection datasets are made to train binary classifiers that\n",
    "detect whether there is a face in an image or not. However, The larger datasets mentioned in Sharma's articles were also either unavailable, or not appropriately labelled. So,\n",
    "we used the following code to scrape images from Google Image search. Our \"scrapper\" grabs the images from the site and uses the cascade classifier to detect and generating new images that only contained the face.\n",
    "It then saves the images into directories created using the search term.\n",
    "\n",
    "*Code for scrapper*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import cv2 as cv\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "DRIVER_PATH = \"./chromedriver\"\n",
    "\n",
    "class FaceScraper:\n",
    "\n",
    "    def __init__(self, path_to_driver=DRIVER_PATH, path_to_face_model=\"./face_detector.xml\", path_to_eye_model=\"./eye_detector.xml\"):\n",
    "\n",
    "        self._img_urls = dict()\n",
    "        self._images = dict()\n",
    "        self._wdriver_path = DRIVER_PATH\n",
    "        self._face_cascade = cv.CascadeClassifier()\n",
    "        self._eye_cascade = cv.CascadeClassifier()\n",
    "\n",
    "        #-- 1. Load the cascades\n",
    "        if not self._face_cascade.load(path_to_face_model):\n",
    "            print('--(!)Error loading face cascade')\n",
    "            exit(0)\n",
    "        if not self._eye_cascade.load(path_to_eye_model):\n",
    "            print('--(!)Error loading eye cascade')\n",
    "            exit(0)\n",
    "\n",
    "    def getImgUrls(self, search_terms=[\"smiling\", \"sad\", \"surprised\", \"angry\", \"neutral\", \"disgust\"], max_num_links = 100, ):\n",
    "        if (search_terms != None):\n",
    "            self._search_terms = search_terms\n",
    "\n",
    "        wd = webdriver.Chrome(executable_path = DRIVER_PATH)\n",
    "\n",
    "        for term in self._search_terms:\n",
    "            self._img_urls[term] = self._fetch_image_urls(term, wd, max_links_to_fetch=max_num_links, sleep_between_interactions=0.1)\n",
    "\n",
    "        wd.quit()\n",
    "\n",
    "    def extractFaces(self):\n",
    "        if len(self._img_urls.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        for label in self._img_urls.keys():\n",
    "            results = []\n",
    "            print(\"Extracting label: %s\\n\" % label)\n",
    "\n",
    "            i = -1\n",
    "            for url in self._img_urls[label]:\n",
    "                try:\n",
    "                    i += 1\n",
    "                    print(\"  Extracting label: {} no: {}; url: {}\".format(label, i, url))\n",
    "                    resp = requests.get(url, stream=True).raw\n",
    "                    print(\"\\tGrabbed image from server\")\n",
    "                    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "                    print(\"\\tConverted to an array\")\n",
    "                    image = cv.imdecode(image, cv.IMREAD_COLOR)\n",
    "                    print(\"\\tDecoded image\")\n",
    "\n",
    "                    print(\"\\tGetting faces\")\n",
    "                    for face in self._detectFace(image):\n",
    "                        results.append(face)\n",
    "                except:\n",
    "                    print(\"    error: couldn't extract faces for url\")\n",
    "                    continue\n",
    "\n",
    "            self._images[label] = results\n",
    "\n",
    "    def saveCropped(self, parent_dir=os.getcwd(), image_type = \"png\"):\n",
    "        if len(self._images.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        path_to_srcapped = parent_dir + \"/scrapped_images\"\n",
    "        i = 1\n",
    "        while (os.path.exists(path_to_srcapped)):\n",
    "            path_to_srcapped = path_to_srcapped + \"_\" + str(i)\n",
    "            i += 1\n",
    "        print (\"Saving to %s\" % path_to_srcapped)\n",
    "        os.mkdir(path_to_srcapped)\n",
    "        for label in self._images:\n",
    "            try:\n",
    "                print(\"Exteracting for %s\" % label)\n",
    "                labelDir = path_to_srcapped + \"/\" + label\n",
    "                os.mkdir(labelDir)\n",
    "            except OSError:\n",
    "                print(\"Unable to write images under %s label\\n\" % label)\n",
    "                continue\n",
    "\n",
    "            for index in range(len(self._images[label])):\n",
    "                image_path =  labelDir + \"/\" + label + \"_\" + str(index) + \".\" + image_type\n",
    "                cv.imwrite(image_path, self._images[label][index])\n",
    "                try:\n",
    "                    print(\"Progress: %.2f%\" % 100 * index/len(self._images[label]))\n",
    "                except:\n",
    "                    continue\n",
    "    def _detectFace(self, frame):\n",
    "        print(\"\\t  preprocessing image\", end=\"... \")\n",
    "        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv.equalizeHist(frame_gray)\n",
    "        #-- Detect faces and eyes\n",
    "        print(\"Detecting faces image\", end=\"... \")\n",
    "        faces = self._face_cascade.detectMultiScale(frame_gray)\n",
    "        eyes = self._eye_cascade.detectMultiScale(frame_gray)\n",
    "        print(\"veryfiying\", end=\"... \")\n",
    "        real_faces = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            # x_min, y_min, x_max, y_max = x, y, x + w, y + h\n",
    "            # for (x_eye, y_eye, w_eye, h_eye) in eyes:\n",
    "            #     if (x_min <= x_eye and x_eye <= x_max) and (y_min <= y_eye and y_eye <= y_max):\n",
    "            #         real_faces.append((x, y, w, h))\n",
    "            #         break\n",
    "            real_faces.append((x, y, w, h))\n",
    "            if len(real_faces) > 5:\n",
    "                break\n",
    "        print(\"\\t  Done!\")\n",
    "        return [frame[y:y+h,x:x+w] for (x,y,w,h) in real_faces]\n",
    "\n",
    "    # source\n",
    "    # https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "    def _fetch_image_urls(self, query, wd, max_links_to_fetch, sleep_between_interactions=1):\n",
    "        def scroll_to_end(wd):\n",
    "            wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # build the google query\n",
    "        search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "        # load the page\n",
    "        wd.get(search_url.format(q=query))\n",
    "\n",
    "        image_urls = set()\n",
    "        image_count = 0\n",
    "        results_start = 0\n",
    "        while image_count < max_links_to_fetch:\n",
    "            scroll_to_end(wd)\n",
    "\n",
    "            # get all image thumbnail results\n",
    "            thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "            number_results = len(thumbnail_results)\n",
    "\n",
    "            print(\"Found: {0} search results. Extracting links from {1}:{0}\".format(number_results, results_start))\n",
    "\n",
    "            for img in thumbnail_results[results_start:number_results]:\n",
    "                # try to click every thumbnail such that we can get the real image behind it\n",
    "                try:\n",
    "                    img.click()\n",
    "                    time.sleep(sleep_between_interactions)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # extract image urls\n",
    "                actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "                for actual_image in actual_images:\n",
    "                    if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                        image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "                image_count = len(image_urls)\n",
    "\n",
    "                if len(image_urls) >= max_links_to_fetch:\n",
    "                    print(\"Found: {} image links, done!\".format(len(image_urls)))\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "                time.sleep(15)\n",
    "                #return image_urls\n",
    "                load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "                if load_more_button:\n",
    "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "                else:\n",
    "                    return image_urls\n",
    "\n",
    "\n",
    "            # move the result startpoint further down\n",
    "            results_start = len(thumbnail_results)\n",
    "\n",
    "        return image_urls\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    list_of_search_terms = [\n",
    "        \"people at weddings\",\n",
    "        \"depression human face\",\n",
    "        \"people at senate hearing\",\n",
    "        \"disgusted face\",\n",
    "        \"people shocked\"\n",
    "        ]\n",
    "\n",
    "    getFaces = FaceScraper()\n",
    "    getFaces.getImgUrls(list_of_search_terms, 200)\n",
    "    getFaces.extractFaces()\n",
    "    getFaces.saveCropped()\n",
    "\n",
    "# Run scrapper\n",
    "#run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This process increased or training data 3-fold. However, because the images were grabbed from a search engine, they also introduced some unintended bias.\n",
    "\n",
    "#### Recognition\n",
    "Our project's original aim was to deploy the model on a server hosted on a raspberry pi that gets video feeds from a client\n",
    "(a headset). However, to meet the pi's hardware performance restrictions, we decided to run the first stage of our classifier\n",
    "on the client. The client then publishes the cropped faces as Mqtt messages to a broker server.\n",
    "\n",
    "The Raspberry Pi is subscribed to the clients topic. When is gets a message (image) from a client, it grabs the image and\n",
    "publishes a string representing the emotion on the broker server. The client will grab the string an display it on the screen.\n",
    "\n",
    "*Code for client*\n",
    "\n",
    "***DO NOT RUN IN JUPYTER NOTEBOOK:*** The code uses openCV's imshow to display the lablled frame. This function is make the jupyter server crash."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_client():\n",
    "    import cv2\n",
    "    import pickle\n",
    "    import socket\n",
    "    import struct\n",
    "\n",
    "    TCP_IP = '127.0.0.1'\n",
    "    TCP_PORT = 9502\n",
    "    video_file = 'facesVid.webm'\n",
    "\n",
    "    # Receive facial expression labels from the server\n",
    "    def receiveLabels(mySocket):\n",
    "        data = mySocket.recv(1024)\n",
    "        print(str(data))\n",
    "        return str(data)\n",
    "\n",
    "    # detects the faces in a frame\n",
    "    def detectAndDisplay(frame):\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv2.equalizeHist(frame_gray)\n",
    "        #-- Detect faces\n",
    "        faces = face_cascade.detectMultiScale(frame_gray)\n",
    "        face_list = []\n",
    "        i = 0\n",
    "        for (x,y,w,h) in faces:\n",
    "            center = (x + w//2, y + h//2)\n",
    "            face_list.append(frame[y:y+h, x:x+w])\n",
    "\n",
    "        return frame, face_list, faces\n",
    "\n",
    "    face_cascade_name = \"./face_detector.xml\"#args.face_cascade\n",
    "    face_cascade = cv2.CascadeClassifier()\n",
    "\n",
    "    #-- 1. Load the cascades\n",
    "    if not face_cascade.load(face_cascade_name):\n",
    "        print('--(!)Error loading face cascade')\n",
    "        exit(0)\n",
    "\n",
    "    print(\"Starting server...\\n\")\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # establishing a tcp connection\n",
    "    sock.bind((TCP_IP, TCP_PORT))\n",
    "    sock.listen(5)\n",
    "\n",
    "    while True:\n",
    "        (client_socket, client_address) = sock.accept()  # wait for server\n",
    "        print\n",
    "        'connection established with ' + str(client_address)\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        # send frames\n",
    "        while True:\n",
    "            flag, frame = cap.read()\n",
    "            labels = []\n",
    "            if flag:\n",
    "                frame_labelled, face_list, bounds = detectAndDisplay(frame)\n",
    "\n",
    "                for i in range(len(face_list)):\n",
    "                    a_face = face_list[i]\n",
    "                    a_face = pickle.dumps(a_face)\n",
    "                    size = len(a_face)\n",
    "                    p = struct.pack('I', size)\n",
    "                    a_face = p + a_face\n",
    "                    client_socket.sendall(a_face)\n",
    "\n",
    "                    label = receiveLabels(client_socket)\n",
    "\n",
    "                    frame = cv2.rectangle(\n",
    "                        frame,\n",
    "                        (bounds[i][0], bounds[i][1]),\n",
    "                        (bounds[i][0] + bounds[i][2], bounds[i][1] + bounds[i][3]),\n",
    "                        (0, 0, 0),\n",
    "                        1\n",
    "                    )\n",
    "                    frame = cv2.putText(frame, label, (bounds[i][0], bounds[i][1] + bounds[i][3] + 5), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "            else:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame - 1)\n",
    "\n",
    "            if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "                size = 10\n",
    "                p = struct.pack(\"I\", size)\n",
    "                client_socket.send(p)\n",
    "                client_socket.send('')\n",
    "                break\n",
    "\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "# Run client\n",
    "# run_client()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: The client uses server sockets. This is an unintended mistake. It will be fixed soon.*\n",
    "\n",
    "The server then uses the model trained on Google Colab to reply with a string\n",
    "containing the two most likely facial expressions.\n",
    "\n",
    "*Code for the server*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_server():\n",
    "    import cv2\n",
    "    import socket\n",
    "    import struct\n",
    "    import pickle\n",
    "    import keras\n",
    "    import numpy\n",
    "    import tensorflow\n",
    "    import math\n",
    "\n",
    "    print(\"Geting model files...\")\n",
    "    # # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    print(\"Loading models...\")\n",
    "    loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # #print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print(\"Model compiled\")\n",
    "\n",
    "    TCP_IP = '153.106.213.22'\n",
    "    TCP_PORT = 9502\n",
    "    server_address = (TCP_IP, TCP_PORT)\n",
    "    i = 0\n",
    "\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.connect((TCP_IP, TCP_PORT))\n",
    "    data = b''\n",
    "    payload_size = struct.calcsize(\"I\")\n",
    "\n",
    "    def getframe(data):\n",
    "        while len(data) < payload_size:\n",
    "            data += sock.recv(4096)\n",
    "        packed_msg_size = data[:payload_size]\n",
    "        data = data[payload_size:]\n",
    "        msg_size = struct.unpack(\"I\", packed_msg_size)[0]\n",
    "        while len(data) < msg_size:\n",
    "            data += sock.recv(4096)\n",
    "        frame_data = data[:msg_size]\n",
    "        data = data[msg_size:]\n",
    "\n",
    "        if frame_data == b'':\n",
    "            return -1, data, None\n",
    "\n",
    "        return 0, data, pickle.loads(frame_data)\n",
    "\n",
    "    # send feed\n",
    "    def sendLabel(socket_, prediction):\n",
    "        emotions = [\"neutral\", \"smiling\", \"sad\", \"surprise-shock\", \"angry\", \"disgusted\", \"fearful\"]\n",
    "\n",
    "        response = emotions[prediction.index(max(prediction))]\n",
    "        prediction.pop(prediction.index(max(prediction)))\n",
    "        if max(prediction) > 0.5:\n",
    "            response = response + \" / \" + emotions[prediction.index(max(prediction))]\n",
    "\n",
    "        socket_.send(bytearray(response, \"utf-8\"))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        flag, data, frame = getframe(data)\n",
    "        if (flag == -1):\n",
    "            break\n",
    "        frame = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        predictions = loaded_model.predict(numpy.reshape(frame, (1, 256, 256, 3)))\n",
    "        print(predictions)\n",
    "        sendLabel(sock, predictions[0].tolist())\n",
    "\n",
    "    sock.close()\n",
    "\n",
    "# Run server\n",
    "# run_server()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: The server uses client sockets. This is an unintended mistake. It will be fixed soon.*\n",
    "\n",
    "### Results\n",
    "*Please watch recognition_trial.mp4 for a demonstration*\n",
    "\n",
    "Overall, our project was a failure. Significant challenges in creating our training dataset heavily influenced our model performance; Infrastructure problems in unity prevented us\n",
    "from integrating our project with the AR headset and limited us project to a facial expression recognizer that spans two computers.\n",
    "\n",
    "Even if our model performed just as well as Sharma's model with an 85% accuracy, it did not have enough training data to understand the\n",
    "neuances of facial expressions. For example, the model has a hard time differentiating between the shock and anger. This is because the datapoints\n",
    " for both expressions had people opening their mouths in them.\n",
    "\n",
    "Additionally, our models were significantly influenced by the peculiarities of our training data. Having used phrases that associate with the emotions,\n",
    "our images had similarities inherent to that phrase. (For example, if we search \"syrian war\" for sad, the resulting images would show the extreme forms of sadness.)\n",
    "We believe this made our model make unhelpful associations between the similarities of the images and the emotion. For example, one of the phrases used to search for images\n",
    "of neutral facial expression was \"passport photo.\" This meant that the pictures had eyes looking directly as the camera. This is reflected in our observations. Frames in which a\n",
    "subject is looking directly at the camera are labelled as neutral.\n",
    "\n",
    "### Implications\n",
    "\n",
    "There obvious ethical issues regarding applications which capture video and transmit it elsewhere via internet. Were this\n",
    " to be a product the users purchased, there would need to be specific security and privacy assurances. Though we never\n",
    " achieved it, our original goal as to have the facial running of an AR headset as the client. This would be a prototype,\n",
    " with, in the future having a product for autistic children. One of the symptoms of autism, particularly in children is\n",
    " difficulty reading facial expressions. Such an application would run on a set of AR glasses, which are indistinguishable\n",
    " from regular glasses these days. A child with autism could have some assistance identifying facial expressions, which\n",
    " could the child learn as well.\n",
    "\n",
    "### Conclusion\n",
    "Facial recognition is growing sub-field in Object-detection research. Our project focused on recognizing facial expressions and\n",
    "classifying their facial expressions. We used openCV's cascading classifier models to find faces in images and trained a CNN\n",
    "to classify the facial expressions. Even if our project was a failure, it still demonstrated the versatility of AI-systems\n",
    "by building a pipline that spans multiple devices. With a fast enough messaging system, this division of labor lets a network\n",
    "of simple machines--like the raspberry pi--can conduct resource intensive AI-tasks as a unit.\n",
    "\n",
    "### Citations\n",
    "\n",
    "Sharma, Gaurav. Real Time Facial Expression Recognition. Real Time Facial Expression Recognition. Medium, n.d. https://medium.com/datadriveninvestor/real-time-facial-expression-recognition-f860dacfeb6a.\n",
    "https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}