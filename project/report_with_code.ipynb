{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Honors Project Report: Distrubuted Facial Expression Recognition\n",
    "#### Samuel Zeleke & Enoch Mwesigwa\n",
    "\n",
    "\n",
    "### Vision\n",
    "Multi-label classification and object detection models have gained significant popularity in recent years. They've become\n",
    "integral components of systems ranging from autonomous cars and security systems, to social media platforms and search\n",
    "engines. Our project is aimed at creating a system that correctly labels facial expressions in real-time on raspberry pi server\n",
    "using a CNN built on keras. Our system should be responsive enough to recognize facial expressions in video streams and light\n",
    "enough to be hosted on a raspberry pi.\n",
    "\n",
    "### Background\n",
    "Object detection involves recognizing multiple objects at different position in the image. Unfortunately, regular\n",
    "CNNs cannot (at least not \"normally\") solve this problem: their architecture only allows inputs and outputs\n",
    "with fixed sizes. So, they are restricted to merely labelling images.\n",
    "\n",
    "There have been several attempts to go around this problem. The most significant ones are using R-CNN and YOLO. [R-CNN](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)\n",
    "and its decedents use additional pre-processing to generate thousands of candidate regions (a.k.a. proposals) in an image\n",
    "and pass each region to a CNN for classification. Obviously, this is a very resource-intensive process, (training in\n",
    "some architectures takes days) and it's not fast enough for real-time object recognition. [YOLO](https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193), on the other hand, uses a single (very) deep CNN to\n",
    "both recognize regions of interest and to classify those regions. This method is several times faster and more\n",
    "efficient than R-CNNs. Unfortunately, the architecture needs a lot of data for training and uses NN layers we were not familiar with.\n",
    "\n",
    "So, building on Gurav Sharma's article \"[Real Time Facial Recognition](https:/medium.com/datadriveninvestor/real-time-facial-expression-recognition-f860dacfeb6a)\",\n",
    "we chose to create a simpler system that combines openCV's trained cascade classifiers to extract the\n",
    "faces and trained a small NN to classify the facial expressions. This largely avoids R-CNNs inefficiencies and significantly reduces the\n",
    "size of training data we need to get decent predictions.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Training\n",
    "Like R-CNNs, our system divides the facial expression task into two stages. In the first stage, we use openCV's pre-trained cascade classifiers\n",
    "to find regions containing faces. For the second stage, Sharma recommends taking advantage of the Keras' pretrained models using transfer learning.\n",
    "(Transfer learning involves using layers from a model trained for a different dataset. It let's the recepient model to take advantage of the \"donor\"\n",
    "model's training by using its weights for predictions.) Unfortunately, we didn't have enough training data to achieve significant training set accuracy.\n",
    "Additionally, the resulting models were taking a lot of storage. Instead, we adopted the architecture we used for the fashion mnist homework to build a small\n",
    "CNN that classified facial expressions.\n",
    "\n",
    "*Code for second-stage model training*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(batch_size = 10):\n",
    "    import os\n",
    "    import zipfile\n",
    "    import cv2\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import layers\n",
    "    # import matplotlib.pyplot as plt\n",
    "    import numpy\n",
    "\n",
    "    # unzipfile\n",
    "    with zipfile.ZipFile(\"/content/drive/My Drive/tif_extended.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./trainingSrc\")\n",
    "\n",
    "    PATH = \"./trainingSrc/tif_extended\"\n",
    "    CLASSES = os.listdir(PATH)\n",
    "\n",
    "    image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split = 0.2)\n",
    "    images = image_generator.flow_from_directory(\n",
    "        PATH,\n",
    "        batch_size = 1,\n",
    "        target_size = (256, 256),\n",
    "    )\n",
    "    sample, label = next(images)\n",
    "    feature_dataframe = []\n",
    "    target_dataframe = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "      feature_dataframe.append(images[i][0][0])\n",
    "      target_dataframe.append(images[i][1][0])\n",
    "\n",
    "    feature_dataframe = numpy.array(feature_dataframe)\n",
    "    target_dataframe = numpy.array(target_dataframe)\n",
    "\n",
    "    # create network\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # input and first convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(30, 2, activation=\"relu\", input_shape = (256, 256, 3)))\n",
    "    model.add(keras.layers.Conv2D(60, kernel_size=5, strides=(2, 2), activation=\"relu\"))#(60, 5, stri activation=\"relu\"))\n",
    "    # input and second convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(30, 3, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    # input and third convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    #flatten\n",
    "    model.add(keras.layers.Flatten())\n",
    "    # three dense layers\n",
    "    model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(28, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(7, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "    x = feature_dataframe,\n",
    "    y = target_dataframe,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding useful training data for our project was a challenging process. Most face detection datasets are made to train binary classifiers that\n",
    "detect whether there is a face in an image or not. Additionally, the larger datasets mentioned in Sharma's article were neither available nor improperly labelled. So,\n",
    "we used the following code to scrape images from Google Image search. Our scrapper use's selenium's webdriver to grab the images\n",
    "from the site and uses the cascade classifier to find regions that contain faces and generate new images for those regions.\n",
    "It then saves the images into directories created using the search term.\n",
    "\n",
    "*Code for scrapper*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import cv2 as cv\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "DRIVER_PATH = \"./chromedriver\"\n",
    "\n",
    "class FaceScraper:\n",
    "\n",
    "    def __init__(self, path_to_driver=DRIVER_PATH, path_to_face_model=\"./face_detector.xml\", path_to_eye_model=\"./eye_detector.xml\"):\n",
    "\n",
    "        self._img_urls = dict()\n",
    "        self._images = dict()\n",
    "        self._wdriver_path = DRIVER_PATH\n",
    "        self._face_cascade = cv.CascadeClassifier()\n",
    "        self._eye_cascade = cv.CascadeClassifier()\n",
    "\n",
    "        #-- 1. Load the cascades\n",
    "        if not self._face_cascade.load(path_to_face_model):\n",
    "            print('--(!)Error loading face cascade')\n",
    "            exit(0)\n",
    "        if not self._eye_cascade.load(path_to_eye_model):\n",
    "            print('--(!)Error loading eye cascade')\n",
    "            exit(0)\n",
    "\n",
    "    def getImgUrls(self, search_terms=[\"smiling\", \"sad\", \"surprised\", \"angry\", \"neutral\", \"disgust\"], max_num_links = 100, ):\n",
    "        if (search_terms != None):\n",
    "            self._search_terms = search_terms\n",
    "\n",
    "        wd = webdriver.Chrome(executable_path = DRIVER_PATH)\n",
    "\n",
    "        for term in self._search_terms:\n",
    "            self._img_urls[term] = self._fetch_image_urls(term, wd, max_links_to_fetch=max_num_links, sleep_between_interactions=0.1)\n",
    "\n",
    "        wd.quit()\n",
    "\n",
    "    def extractFaces(self):\n",
    "        if len(self._img_urls.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        for label in self._img_urls.keys():\n",
    "            results = []\n",
    "            print(\"Extracting label: %s\\n\" % label)\n",
    "\n",
    "            i = -1\n",
    "            for url in self._img_urls[label]:\n",
    "                try:\n",
    "                    i += 1\n",
    "                    print(\"  Extracting label: {} no: {}; url: {}\".format(label, i, url))\n",
    "                    resp = requests.get(url, stream=True).raw\n",
    "                    print(\"\\tGrabbed image from server\")\n",
    "                    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "                    print(\"\\tConverted to an array\")\n",
    "                    image = cv.imdecode(image, cv.IMREAD_COLOR)\n",
    "                    print(\"\\tDecoded image\")\n",
    "\n",
    "                    print(\"\\tGetting faces\")\n",
    "                    for face in self._detectFace(image):\n",
    "                        results.append(face)\n",
    "                except:\n",
    "                    print(\"    error: couldn't extract faces for url\")\n",
    "                    continue\n",
    "\n",
    "            self._images[label] = results\n",
    "\n",
    "    def saveCropped(self, parent_dir=os.getcwd(), image_type = \"png\"):\n",
    "        if len(self._images.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        path_to_srcapped = parent_dir + \"/scrapped_images\"\n",
    "        i = 1\n",
    "        while (os.path.exists(path_to_srcapped)):\n",
    "            path_to_srcapped = path_to_srcapped + \"_\" + str(i)\n",
    "            i += 1\n",
    "        print (\"Saving to %s\" % path_to_srcapped)\n",
    "        os.mkdir(path_to_srcapped)\n",
    "        for label in self._images:\n",
    "            try:\n",
    "                print(\"Exteracting for %s\" % label)\n",
    "                labelDir = path_to_srcapped + \"/\" + label\n",
    "                os.mkdir(labelDir)\n",
    "            except OSError:\n",
    "                print(\"Unable to write images under %s label\\n\" % label)\n",
    "                continue\n",
    "\n",
    "            for index in range(len(self._images[label])):\n",
    "                image_path =  labelDir + \"/\" + label + \"_\" + str(index) + \".\" + image_type\n",
    "                cv.imwrite(image_path, self._images[label][index])\n",
    "                try:\n",
    "                    print(\"Progress: %.2f%\" % 100 * index/len(self._images[label]))\n",
    "                except:\n",
    "                    continue\n",
    "    def _detectFace(self, frame):\n",
    "        print(\"\\t  preprocessing image\", end=\"... \")\n",
    "        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv.equalizeHist(frame_gray)\n",
    "        #-- Detect faces and eyes\n",
    "        print(\"Detecting faces image\", end=\"... \")\n",
    "        faces = self._face_cascade.detectMultiScale(frame_gray)\n",
    "        eyes = self._eye_cascade.detectMultiScale(frame_gray)\n",
    "        print(\"veryfiying\", end=\"... \")\n",
    "        real_faces = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            # x_min, y_min, x_max, y_max = x, y, x + w, y + h\n",
    "            # for (x_eye, y_eye, w_eye, h_eye) in eyes:\n",
    "            #     if (x_min <= x_eye and x_eye <= x_max) and (y_min <= y_eye and y_eye <= y_max):\n",
    "            #         real_faces.append((x, y, w, h))\n",
    "            #         break\n",
    "            real_faces.append((x, y, w, h))\n",
    "            if len(real_faces) > 5:\n",
    "                break\n",
    "        print(\"\\t  Done!\")\n",
    "        return [frame[y:y+h,x:x+w] for (x,y,w,h) in real_faces]\n",
    "\n",
    "    # source\n",
    "    # https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "    def _fetch_image_urls(self, query, wd, max_links_to_fetch, sleep_between_interactions=1):\n",
    "        def scroll_to_end(wd):\n",
    "            wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # build the google query\n",
    "        search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "        # load the page\n",
    "        wd.get(search_url.format(q=query))\n",
    "\n",
    "        image_urls = set()\n",
    "        image_count = 0\n",
    "        results_start = 0\n",
    "        while image_count < max_links_to_fetch:\n",
    "            scroll_to_end(wd)\n",
    "\n",
    "            # get all image thumbnail results\n",
    "            thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "            number_results = len(thumbnail_results)\n",
    "\n",
    "            print(\"Found: {0} search results. Extracting links from {1}:{0}\".format(number_results, results_start))\n",
    "\n",
    "            for img in thumbnail_results[results_start:number_results]:\n",
    "                # try to click every thumbnail such that we can get the real image behind it\n",
    "                try:\n",
    "                    img.click()\n",
    "                    time.sleep(sleep_between_interactions)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # extract image urls\n",
    "                actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "                for actual_image in actual_images:\n",
    "                    if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                        image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "                image_count = len(image_urls)\n",
    "\n",
    "                if len(image_urls) >= max_links_to_fetch:\n",
    "                    print(\"Found: {} image links, done!\".format(len(image_urls)))\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "                time.sleep(15)\n",
    "                #return image_urls\n",
    "                load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "                if load_more_button:\n",
    "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "                else:\n",
    "                    return image_urls\n",
    "\n",
    "\n",
    "            # move the result startpoint further down\n",
    "            results_start = len(thumbnail_results)\n",
    "\n",
    "        return image_urls\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    list_of_search_terms = [\n",
    "        \"people at weddings\",\n",
    "        \"depression human face\",\n",
    "        \"people at senate hearing\",\n",
    "        \"disgusted face\",\n",
    "        \"people shocked\"\n",
    "        ]\n",
    "\n",
    "    getFaces = FaceScraper()\n",
    "    getFaces.getImgUrls(list_of_search_terms, 200)\n",
    "    getFaces.extractFaces()\n",
    "    getFaces.saveCropped()\n",
    "\n",
    "# Run scrapper\n",
    "#run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This process increased or training data 3-fold. However, because the images were grabbed from a search engine, they also introduced some unintended bias.\n",
    "\n",
    "#### Recognition\n",
    "Our project's original aim was to deploy the model on a server hosted on a raspberry pi that gets video feeds from a\n",
    "client (a headset). However, to meet the pi's hardware performance restrictions, we decided to run the first stage of our\n",
    " classifier on the client. The client would only send and image once it had determined that there was an face in the\n",
    " picture. This also severs to decrease the number of images being sent to and from the client. Then, the client publishes\n",
    " the cropped faces as MQTT messages to a broker server under a specific thread.\n",
    "\n",
    "*Code for client*\n",
    "\n",
    "***DO NOT RUN IN JUPYTER NOTEBOOK:*** The code uses openCV's imshow to display the labelled frame. This function usually crashes jupyter's server."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kivy.app import App\n",
    "from kivy.uix.widget import Widget\n",
    "from kivy.lang import Builder\n",
    "from kivy.uix.gridlayout import GridLayout\n",
    "from kivy.uix.image import Image\n",
    "from kivy.clock import Clock\n",
    "from kivy.properties import ListProperty\n",
    "from kivy.graphics.texture import Texture\n",
    "import time\n",
    "from threading import Thread, Event, Lock\n",
    "import cv2\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import struct\n",
    "import paho.mqtt.client as mqtt\n",
    "from kivy.uix.label import Label\n",
    "from kivy.graphics import Color, Rectangle\n",
    "from kivy.core.window import Window\n",
    "\n",
    "\n",
    "Builder.load_string('''\n",
    "<FormattedLabel>:\n",
    "    text: root.text\n",
    "    fontSize: 70\n",
    "    color: (0, 0.2, .4, 1)\n",
    "    size_hint: 1, 0.04\n",
    "    background_color: 0,0,0,1\n",
    "    '''\n",
    "    )\n",
    "'''\n",
    "<Divider>:\n",
    "color: (1, 0.2, .4, .2)\n",
    "background_color: 1,0,0,0\n",
    "'''\n",
    "# class for managing the label display\n",
    "class FormattedLabel(Label):\n",
    "    background_color = ListProperty()\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FormattedLabel, self).__init__()\n",
    "        Clock.schedule_once(lambda dt: self.initialize_widget(), 0.002)\n",
    "\n",
    "    def initialize_widget(self):\n",
    "        self.canvas.before.add(Color(self.background_color))\n",
    "        self.canvas.before.add(Rectangle(pos=self.pos, size=self.size))\n",
    "        self.text_size = self.size\n",
    "        self.text =\"Searching for face...\"\n",
    "        self.halign = 'center'\n",
    "        self.valign = 'top'\n",
    "        self.bold = True\n",
    "\n",
    "    def update(self, text):\n",
    "        self.text = text\n",
    "\n",
    "class Divider(Widget):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Divider, self).__init__(**kwargs)\n",
    "\n",
    "        with self.canvas:\n",
    "            Color(1, 0, 0, 1)  # set the colour to red\n",
    "            self.rect = Rectangle(pos=self.center,\n",
    "                                  size=(Window.size[0],\n",
    "                                        self.height/8.))\n",
    "    def update(self, color):\n",
    "        with self.canvas:\n",
    "            Color(color)  # set the colour to red\n",
    "            self.rect = Rectangle(pos=self.top,\n",
    "                                  size=(Window.size[0],\n",
    "                                        self.height/8.))\n",
    "\n",
    "#class for displaying camerafeed\n",
    "class KivyCamera(Image):\n",
    "    def __init__(self, capture, fps, **kwargs):\n",
    "        super(KivyCamera, self).__init__(**kwargs)\n",
    "        self.capture = capture\n",
    "        Clock.schedule_interval(self.update, 1.0 / fps) # updates in response to framerate\n",
    "\n",
    "    def update(self, dt):\n",
    "        ret, frame = self.capture.read()\n",
    "        if ret:\n",
    "            buf1 = cv2.flip(frame, 0)\n",
    "            buf = buf1.tostring()\n",
    "            image_texture = Texture.create(\n",
    "                size=(frame.shape[1], frame.shape[0]), colorfmt='bgr')\n",
    "            image_texture.blit_buffer(buf, colorfmt='bgr', bufferfmt='ubyte')\n",
    "            self.texture = image_texture\n",
    "\n",
    "\n",
    "#main up\n",
    "class RecogApp(App):\n",
    "\n",
    "    def build(self):\n",
    "        self.capture = cv2.VideoCapture(0)\n",
    "        self.my_camera = KivyCamera(capture=self.capture, fps=30)\n",
    "        self.total = GridLayout(rows=2)\n",
    "        self.label = FormattedLabel()\n",
    "       # self.divider = Divider()\n",
    "        self.total.add_widget(self.my_camera)\n",
    "        #self.total.add_widget(self.divider)\n",
    "        self.total.add_widget(self.label)\n",
    "        return self.total\n",
    "\n",
    "    def on_stop(self):\n",
    "        print(\"*** closing...\")\n",
    "        exit.set()\n",
    "        cam.join()\n",
    "\n",
    "\n",
    "    def on_start(self):\n",
    "        QOS = 0\n",
    "        BROKER = 'test.mosquitto.org'\n",
    "        PORT = 1883\n",
    "\n",
    "        def on_connect(client, userdata, rc, *extra_params):\n",
    "            print('Connected with result code='+str(rc))\n",
    "            client.subscribe(\"aiproj/facrecog/response\", qos=QOS)\n",
    "\n",
    "        def send_image(face, frame):\n",
    "            (result, num) = client.publish('aiproj/facrecog/image', face, qos=QOS)\n",
    "            print(result, num)\n",
    "            if result != 0:\n",
    "                print('PUBLISH returned error:', result)\n",
    "\n",
    "        def on_message(client, data, msg):\n",
    "            if msg.topic == \"aiproj/facrecog/response\":\n",
    "                if msg:\n",
    "                    print(\"recieved message: \", str(msg.payload.decode()))\n",
    "                    self.label.update(\"Emotion: \"+ str(msg.payload.decode()))\n",
    "\n",
    "        def detectAndDisplay(frame):\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_gray = cv2.equalizeHist(frame_gray)\n",
    "            faces = face_cascade.detectMultiScale(frame_gray)\n",
    "            face_list = []\n",
    "            i = 0\n",
    "            for (x,y,w,h) in faces:\n",
    "                center = (x + w//2, y + h//2)\n",
    "                face_list.append(frame[y:y+h, x:x+w])\n",
    "            return frame, face_list, faces\n",
    "\n",
    "        def poll():\n",
    "            while True:\n",
    "                flag, frame = self.capture.read()\n",
    "                labels = []\n",
    "                if flag:\n",
    "                    frame_labelled, face_list, bounds = detectAndDisplay(frame)\n",
    "                    if len(face_list) == 0:\n",
    "                        self.label.update(\"Searching for face...\")\n",
    "                    for i in range(len(face_list)):\n",
    "                        a_face = face_list[i]\n",
    "                        a_face = pickle.dumps(a_face)\n",
    "                        send_image(a_face, frame)\n",
    "                if exit.is_set():\n",
    "                    break\n",
    "            #time.sleep(.5)\n",
    "\n",
    "        #get the dimintions of the image\n",
    "        pos_frame = self.capture.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        face_cascade_name = 'haarcascade_frontalface_default.xml'\n",
    "        face_cascade = cv2.CascadeClassifier()\n",
    "        face_cascade.load(face_cascade_name)\n",
    "\n",
    "        '''\n",
    "        server = Thread(target=start_server)\n",
    "        start the thread\n",
    "        server.daemon = True\n",
    "        server.start()\n",
    "        '''\n",
    "        global client\n",
    "        global exit\n",
    "        global cam\n",
    "        exit = Event()\n",
    "        client = mqtt.Client()\n",
    "        client.on_connect = on_connect\n",
    "        client.on_message = on_message\n",
    "        client.connect(BROKER, PORT, 60)\n",
    "        cam = Thread(target=poll)\n",
    "        client.loop_start()\n",
    "        cam.start()\n",
    "\n",
    "# Run client\n",
    "app = RecogApp()\n",
    "# app.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Raspberry Pi (server) is subscribed to the client’s topic. When client publisheds a message (image), the Pi grabs\n",
    "the image and puts it on a queue. The pi has a user-specified number of threads read to pull an image of queue, use the\n",
    "model to predict the facial expression, then converts it to byes and sends it to a the server under a different thread.\n",
    "The client will be subscribed to that thread, once it receives a message, it will display it on the screen.\n",
    "\n",
    "*Code for the server*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_server():\n",
    "    import json\n",
    "    import sys\n",
    "    from threading import Thread, Event, Lock\n",
    "    from queue import Queue\n",
    "    import time\n",
    "    import struct\n",
    "    import pickle\n",
    "    from tensorflow import keras\n",
    "    import numpy\n",
    "    import tensorflow as tf\n",
    "    import paho.mqtt.client as mqtt\n",
    "    import cv2 as cv\n",
    "\n",
    "    QOS = 0\n",
    "    BROKER = 'test.mosquitto.org'\n",
    "    PORT = 1883\n",
    "\n",
    "    #thread class to process data\n",
    "    class myThread (Thread):\n",
    "       def __init__(self, threadID, name, client):\n",
    "          Thread.__init__(self)\n",
    "          self.threadID = threadID\n",
    "          self.name = name\n",
    "          self.client =client\n",
    "\n",
    "       def run(self):\n",
    "          print (\"Starting \" + self.name)\n",
    "          process_data(self.name, self.client, model)\n",
    "          print (\"Exiting \" + self.name)\n",
    "\n",
    "    #call back for mqtt client\n",
    "    def on_message(client, data, msg):\n",
    "        if msg.topic == \"aiproj/facrecog/image\":\n",
    "            if msg:\n",
    "                print(\"message\")\n",
    "                print(\"item\")\n",
    "                workQueue.put(msg.payload)\n",
    "                print(\"released lock\")\n",
    "                print(len(workQueue))\n",
    "\n",
    "    # call back for mqtt client\n",
    "    def on_connect(client, userdata, rc, *extra_params):\n",
    "       print('Connected with result code='+str(rc))\n",
    "       client.subscribe(\"aiproj/facrecog/image\", qos=QOS)\n",
    "\n",
    "    #closes client and terminates threads\n",
    "    def kill_command(threads, client):\n",
    "        print(\"closing\")\n",
    "        client.close()\n",
    "        exitFlag.set()\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "    # sends data to client\n",
    "    def send_response(client, message):\n",
    "        (result, num) = client.publish('aiproj/facrecog/response', message, qos=QOS)\n",
    "        print(\"sent response:\", message)\n",
    "        if result != 0:\n",
    "            print('PUBLISH returned error:', result)\n",
    "\n",
    "    #uses model to get prediction\n",
    "    def predict(data, model):\n",
    "        frame = cv.resize(data, (256, 256), interpolation=cv.INTER_AREA)\n",
    "        with session.graph.as_default():\n",
    "            keras.backend.set_session(session)\n",
    "            predictions = model.predict(numpy.reshape(frame, (1, 256, 256, 3)))\n",
    "            print(predictions)\n",
    "            return predictions[0].tolist()\n",
    "\n",
    "    #converts the prediction to a string\n",
    "    def getLabel(prediction):\n",
    "        emotions = [\"neutral\", \"smiling\", \"sad\", \"surprise-shock\", \"angry\", \"disgusted\", \"fearful\"]\n",
    "        response = emotions[prediction.index(max(prediction))]\n",
    "        prediction.pop(prediction.index(max(prediction)))\n",
    "        if max(prediction) > .5:\n",
    "            response = response + \"/\" + emotions[prediction.index(max(prediction))]\n",
    "        return response\n",
    "\n",
    "     #work function for threads\n",
    "    def process_data(threadName, client, model):\n",
    "        while True:\n",
    "            data = workQueue.get()\n",
    "            if data:\n",
    "                print (threadName, \"processing an image\")\n",
    "                item = pickle.loads(data)\n",
    "                message = getLabel(predict(item, model))\n",
    "                print(threadName, \"processed\", message)\n",
    "                #send precition\n",
    "                sendLock.acquire()\n",
    "                send_response(client, message)\n",
    "                sendLock.release()\n",
    "            else:\n",
    "                print(\"...\")\n",
    "            if exitFlag.is_set():\n",
    "                break\n",
    "            time.sleep(.25)\n",
    "    #\n",
    "    def init(thread_nums):\n",
    "        print(\"*****\")\n",
    "        print(\"Initializing connection...\")\n",
    "        print(\"*****\")\n",
    "        client = mqtt.Client()\n",
    "        threads = []\n",
    "        client.on_connect = on_connect\n",
    "        client.on_message = on_message\n",
    "        client.connect(BROKER, PORT, 60)\n",
    "        for threadID in range(thread_nums):\n",
    "            threads.append(myThread(threadID, \"Thread\"+str(threadID), client))\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        return threads,client\n",
    "\n",
    "\n",
    "\n",
    "    def load_model():\n",
    "        print(\"Geting model files...\")\n",
    "        # # load json and create model\n",
    "        json_file = open('model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        print(\"*****\")\n",
    "        print(\"Loading models...\")\n",
    "        print(\"*****\")\n",
    "        session = tf.Session(graph=tf.Graph())\n",
    "        with session.graph.as_default():\n",
    "            keras.backend.set_session(session)\n",
    "            loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "            loaded_model.load_weights(\"model.h5\")\n",
    "            return loaded_model, session\n",
    "        print(\"*****\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        print(\"*****\")\n",
    "        # # evaluate loaded model on test data\n",
    "        loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #graph = tf.get_default_graph()\n",
    "        print(\"*****\")\n",
    "        print(\"Model compiled\")\n",
    "        print(\"*****\")\n",
    "\n",
    "\n",
    "    global model\n",
    "    global session\n",
    "    global sendLock\n",
    "    global workQueue\n",
    "    model, session =load_model()\n",
    "    exitFlag = Event()\n",
    "    sendLock = Lock()\n",
    "    connected = Event()\n",
    "    workQueue = Queue(30)\n",
    "    threads,client = init(3)\n",
    "    try:\n",
    "        client.loop_start()\n",
    "    except KeyboardInterrupt:\n",
    "        kill_command(threads, client)\n",
    "\n",
    "# Run server\n",
    "# run_server()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "*Please watch ./demo.mov for a demonstration*\n",
    "\n",
    "Overall, our project was a partial success. We were able to deploy our classifier on a raspberry pi and make predictions\n",
    "for frames sent by a client. But the expression-recognition process was a bit slow. This was because drawing manipulating images\n",
    " on the client was very resource intensive.\n",
    "\n",
    "Even if our model performed just as well as Sharma's model with an 85% accuracy, it did not have enough training data to understand the\n",
    "nuances of facial expressions. For example, the model has a hard time differentiating between the shock and anger. This is because the datapoints\n",
    " for both expressions had people opening their mouths in them.\n",
    "\n",
    "Additionally, our models were significantly influenced by the peculiarities of our training data. Having used phrases that associate with the emotions,\n",
    "our images had similarities inherent to that phrase. (For example, if we search \"syrian war\" for sad, the resulting images would show the extreme forms of sadness.)\n",
    "We believe this made our model make unhelpful associations between the similarities of the images and the emotion. For example, one of the phrases used to search for images\n",
    "of neutral facial expression was \"passport photo.\" This meant that the pictures had eyes looking directly as the camera. This is reflected in our observations. Frames in which a\n",
    "subject is looking directly at the camera are labelled as neutral.\n",
    "\n",
    "### Implications\n",
    "\n",
    "There are obvious ethical issues regarding applications which capture video and transmit it elsewhere via internet. Were this\n",
    " to be a product the users purchased, there would need to be specific security and privacy assurances.\n",
    "\n",
    " Though we never achieved it, our original goal as to have the facial running of an AR headset as the client. This would be a prototype,\n",
    " with, in the future having a product for autistic children. One of the symptoms of autism, particularly in children is\n",
    " difficulty reading facial expressions. Such an application would run on a set of AR glasses, which are indistinguishable\n",
    " from regular glasses these days. A child with autism could have some assistance identifying facial expressions, which the child could learn as well.\n",
    "\n",
    "Our project also shows the importance of responsible data collection and training in machine learning. Models reflect their training.\n",
    "In our project, we noticed that the model made unintended associations between some facial features and some facial\n",
    "expressions based on its training data. In \"real\" applications, ignoring bias in the training sets can result in models that make inaccurate predictions\n",
    "that influence peoples lives.\n",
    "\n",
    "### Conclusion\n",
    "Facial recognition is growing sub-field in Object-detection research. Our project focused on recognizing facial expressions and\n",
    "classifying their facial expressions. We used openCV's cascading classifier models to find faces in images and trained a CNN\n",
    "to classify the facial expressions. These two tasks were distributed between a server and a client that communicated using\n",
    "the MQTT messaging protocol. Even if our project was not a complete success, it still demonstrated the potential of facial recognition\n",
    "systems. They can be used as aids to people who need assistance in understanding emotions. They are also versatile. The detection task can be\n",
    "distributed to between multiple devices in a pipeline. With a fast enough messaging system, this division of labor lets a network\n",
    "of simple machines--like the raspberry pi--conduct resource intensive recognition tasks.\n",
    "\n",
    "### Citations\n",
    "\n",
    "[Sharma, Gaurav. Real Time Facial Expression Recognition. Real Time Facial Expression Recognition. Medium, n.d.](https://medium.com/datadriveninvestor/real-time-facial-expression-recognition-f860dacfeb6a.)\n",
    "\n",
    "[Zenodo. The Japanese Female Facial Expression Database.](https://zenodo.org/record/3451524#.Xs5tLPJKiV4)\n",
    "\n",
    "[VISGRAF. FaceDB.](http://app.visgraf.impa.br/database/faces/)\n",
    "\n",
    "[Bosler, Fabian. Image Scraping with Python. Medium. September, 2019.](https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}