{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Honors Project Report: Distrubuted Facial Expression Recognition\n",
    "#### Samuel Zeleke & Enoch Mwesigwa\n",
    "\n",
    "\n",
    "### Vision\n",
    "Multi-label classification and object detection models have gained significant popularity in recent years. They've become\n",
    "integral components of systems ranging from autonomous cars and security systems, to social media platforms and search\n",
    "engines. Our project is aimed at creating a system that correctly labels facial expressions in real-time using an Aryzon headset and\n",
    "CNNs built on keras. We'll use OpenCV's cascade classifier to extract faces from frames, and train a separate CNN classify the facial\n",
    "expression. The output is used to draw a labelled box around faces on display the results on the client. Unfortunately, because of limitations\n",
    "in training data and problems with unity, our project was limited to the server that hosts the classifier and a remote client\n",
    "that sends images for classification.\n",
    "\n",
    "### Background\n",
    "Object detection entails recognizing multiple objects places at different locations in the image. Unfortunately, regular\n",
    "covNets cannot (at least not \"normally\") cannot solve this problem: their architecture only allows inputs and outputs\n",
    "with fixed sizes. So, they are restricted to merely labelling images.\n",
    "\n",
    "There have been several attempts to go around this problem. The most significant ones are using R-CNN and YOLO. [R-CNN](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)\n",
    "and its decedents use additional preprocessing to generate thousands of candidate regions (called \"proposals\") in the images\n",
    "and pass each region to a covNet for classification. Obviously, this is a very resource-intensive process, (training in\n",
    "some architectures takes days) and they are not fast enough for real-time object recognition. [YOLO](https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193), on the other hand, uses a single (very) deep CovNet to\n",
    "both recognize regions of interest and to classify those regions. This method is several times faster and more\n",
    "efficient than R-CNNs. Unfortunately, the architecture needs a lot of data for training and uses NN layers we were not familiar with.\n",
    "\n",
    "So, building on Gurav Sharma's article \"[Real Time Facial Recognition](https:/medium.com/datadriveninvestor/real-time-facial-expression-recognition-f860dacfeb6a)\",\n",
    "we chose to create a simpler system that combines openCV's trained cascade classifiers to extract the\n",
    "faces and trained a small NN to classify the facial expressions. This largely avoids R-CNNs inefficiencies and significantly reduces the\n",
    "size of training data we need to get decent predictions.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Training\n",
    "Like R-CNNs, our system divides the facial expression task into two stages. In the first stage, we use openCV's pre-trained cascade classifiers\n",
    "to find regions containing faces. In the second stage, we use a small CNN to classify the facial expressions. For the second stage,\n",
    "Sharma recommends taking advantage of the power of Keras' pretrained modes using transfer learning. (Transfer learning\n",
    "involves using layers from a model trained for a different dataset. It let's the recepient model to take advantage of the \"donor\" model's training by using its weights for predictions.)\n",
    "Unfortunately, with our limited training data, we were unable to get training set accuracy of 0.04. Additionally, the resulting models were taking a lot of storage. So, instead we adapted the\n",
    "architecture I used for our assignment on the Fashion MNIST dataset. Our CNN performed my better with an validation set accuracy of 0.87. (The simplest solution s the best solution, after all!)\n",
    "\n",
    "*Code for second-stage model training*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(batch_size = 10):\n",
    "    import os\n",
    "    import zipfile\n",
    "    import cv2\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import layers\n",
    "    # import matplotlib.pyplot as plt\n",
    "    import numpy\n",
    "\n",
    "    # unzipfile\n",
    "    with zipfile.ZipFile(\"/content/drive/My Drive/tif_extended.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./trainingSrc\")\n",
    "\n",
    "    PATH = \"./trainingSrc/tif_extended\"\n",
    "    CLASSES = os.listdir(PATH)\n",
    "\n",
    "    image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split = 0.2)\n",
    "    images = image_generator.flow_from_directory(\n",
    "        PATH,\n",
    "        batch_size = 1,\n",
    "        target_size = (256, 256),\n",
    "    )\n",
    "    sample, label = next(images)\n",
    "    feature_dataframe = []\n",
    "    target_dataframe = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "      feature_dataframe.append(images[i][0][0])\n",
    "      target_dataframe.append(images[i][1][0])\n",
    "\n",
    "    feature_dataframe = numpy.array(feature_dataframe)\n",
    "    target_dataframe = numpy.array(target_dataframe)\n",
    "\n",
    "    # create network\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # input and first convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(30, 2, activation=\"relu\", input_shape = (256, 256, 3)))\n",
    "    model.add(keras.layers.Conv2D(60, kernel_size=5, strides=(2, 2), activation=\"relu\"))#(60, 5, stri activation=\"relu\"))\n",
    "    # input and second convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.Conv2D(30, 3, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    # input and third convolution: extract 30 features\n",
    "    model.add(keras.layers.Conv2D(60, 5, activation=\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    #flatten\n",
    "    model.add(keras.layers.Flatten())\n",
    "    # three dense layers\n",
    "    model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(28, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(7, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "    x = feature_dataframe,\n",
    "    y = target_dataframe,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding useful training data for our project was a challenging process.Most face detection datasets are made to train binary classifiers that\n",
    "detect whether there is a face in an image or not. However, The larger datasets mentioned in Sharma's articles were also either unavailable, or not appropriately labelled. So,\n",
    "we used the following code to scrape images from Google Image search. Our \"scrapper\" grabs the images from the site and uses the cascade classifier to detect and generating new images that only contained the face.\n",
    "It then saves the images into directories created using the search term.\n",
    "\n",
    "*Code for scrapper*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import cv2 as cv\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "DRIVER_PATH = \"./chromedriver\"\n",
    "\n",
    "class FaceScraper:\n",
    "\n",
    "    def __init__(self, path_to_driver=DRIVER_PATH, path_to_face_model=\"./face_detector.xml\", path_to_eye_model=\"./eye_detector.xml\"):\n",
    "\n",
    "        self._img_urls = dict()\n",
    "        self._images = dict()\n",
    "        self._wdriver_path = DRIVER_PATH\n",
    "        self._face_cascade = cv.CascadeClassifier()\n",
    "        self._eye_cascade = cv.CascadeClassifier()\n",
    "\n",
    "        #-- 1. Load the cascades\n",
    "        if not self._face_cascade.load(path_to_face_model):\n",
    "            print('--(!)Error loading face cascade')\n",
    "            exit(0)\n",
    "        if not self._eye_cascade.load(path_to_eye_model):\n",
    "            print('--(!)Error loading eye cascade')\n",
    "            exit(0)\n",
    "\n",
    "    def getImgUrls(self, search_terms=[\"smiling\", \"sad\", \"surprised\", \"angry\", \"neutral\", \"disgust\"], max_num_links = 100, ):\n",
    "        if (search_terms != None):\n",
    "            self._search_terms = search_terms\n",
    "\n",
    "        wd = webdriver.Chrome(executable_path = DRIVER_PATH)\n",
    "\n",
    "        for term in self._search_terms:\n",
    "            self._img_urls[term] = self._fetch_image_urls(term, wd, max_links_to_fetch=max_num_links, sleep_between_interactions=0.1)\n",
    "\n",
    "        wd.quit()\n",
    "\n",
    "    def extractFaces(self):\n",
    "        if len(self._img_urls.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        for label in self._img_urls.keys():\n",
    "            results = []\n",
    "            print(\"Extracting label: %s\\n\" % label)\n",
    "\n",
    "            i = -1\n",
    "            for url in self._img_urls[label]:\n",
    "                try:\n",
    "                    i += 1\n",
    "                    print(\"  Extracting label: {} no: {}; url: {}\".format(label, i, url))\n",
    "                    resp = requests.get(url, stream=True).raw\n",
    "                    print(\"\\tGrabbed image from server\")\n",
    "                    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "                    print(\"\\tConverted to an array\")\n",
    "                    image = cv.imdecode(image, cv.IMREAD_COLOR)\n",
    "                    print(\"\\tDecoded image\")\n",
    "\n",
    "                    print(\"\\tGetting faces\")\n",
    "                    for face in self._detectFace(image):\n",
    "                        results.append(face)\n",
    "                except:\n",
    "                    print(\"    error: couldn't extract faces for url\")\n",
    "                    continue\n",
    "\n",
    "            self._images[label] = results\n",
    "\n",
    "    def saveCropped(self, parent_dir=os.getcwd(), image_type = \"png\"):\n",
    "        if len(self._images.keys()) == 0:\n",
    "            raise ValueError(\"No images in object.\")\n",
    "\n",
    "        path_to_srcapped = parent_dir + \"/scrapped_images\"\n",
    "        i = 1\n",
    "        while (os.path.exists(path_to_srcapped)):\n",
    "            path_to_srcapped = path_to_srcapped + \"_\" + str(i)\n",
    "            i += 1\n",
    "        print (\"Saving to %s\" % path_to_srcapped)\n",
    "        os.mkdir(path_to_srcapped)\n",
    "        for label in self._images:\n",
    "            try:\n",
    "                print(\"Exteracting for %s\" % label)\n",
    "                labelDir = path_to_srcapped + \"/\" + label\n",
    "                os.mkdir(labelDir)\n",
    "            except OSError:\n",
    "                print(\"Unable to write images under %s label\\n\" % label)\n",
    "                continue\n",
    "\n",
    "            for index in range(len(self._images[label])):\n",
    "                image_path =  labelDir + \"/\" + label + \"_\" + str(index) + \".\" + image_type\n",
    "                cv.imwrite(image_path, self._images[label][index])\n",
    "                try:\n",
    "                    print(\"Progress: %.2f%\" % 100 * index/len(self._images[label]))\n",
    "                except:\n",
    "                    continue\n",
    "    def _detectFace(self, frame):\n",
    "        print(\"\\t  preprocessing image\", end=\"... \")\n",
    "        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv.equalizeHist(frame_gray)\n",
    "        #-- Detect faces and eyes\n",
    "        print(\"Detecting faces image\", end=\"... \")\n",
    "        faces = self._face_cascade.detectMultiScale(frame_gray)\n",
    "        eyes = self._eye_cascade.detectMultiScale(frame_gray)\n",
    "        print(\"veryfiying\", end=\"... \")\n",
    "        real_faces = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            # x_min, y_min, x_max, y_max = x, y, x + w, y + h\n",
    "            # for (x_eye, y_eye, w_eye, h_eye) in eyes:\n",
    "            #     if (x_min <= x_eye and x_eye <= x_max) and (y_min <= y_eye and y_eye <= y_max):\n",
    "            #         real_faces.append((x, y, w, h))\n",
    "            #         break\n",
    "            real_faces.append((x, y, w, h))\n",
    "            if len(real_faces) > 5:\n",
    "                break\n",
    "        print(\"\\t  Done!\")\n",
    "        return [frame[y:y+h,x:x+w] for (x,y,w,h) in real_faces]\n",
    "\n",
    "    # source\n",
    "    # https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "    def _fetch_image_urls(self, query, wd, max_links_to_fetch, sleep_between_interactions=1):\n",
    "        def scroll_to_end(wd):\n",
    "            wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # build the google query\n",
    "        search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "        # load the page\n",
    "        wd.get(search_url.format(q=query))\n",
    "\n",
    "        image_urls = set()\n",
    "        image_count = 0\n",
    "        results_start = 0\n",
    "        while image_count < max_links_to_fetch:\n",
    "            scroll_to_end(wd)\n",
    "\n",
    "            # get all image thumbnail results\n",
    "            thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "            number_results = len(thumbnail_results)\n",
    "\n",
    "            print(\"Found: {0} search results. Extracting links from {1}:{0}\".format(number_results, results_start))\n",
    "\n",
    "            for img in thumbnail_results[results_start:number_results]:\n",
    "                # try to click every thumbnail such that we can get the real image behind it\n",
    "                try:\n",
    "                    img.click()\n",
    "                    time.sleep(sleep_between_interactions)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # extract image urls\n",
    "                actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "                for actual_image in actual_images:\n",
    "                    if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                        image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "                image_count = len(image_urls)\n",
    "\n",
    "                if len(image_urls) >= max_links_to_fetch:\n",
    "                    print(\"Found: {} image links, done!\".format(len(image_urls)))\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "                time.sleep(15)\n",
    "                #return image_urls\n",
    "                load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "                if load_more_button:\n",
    "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "                else:\n",
    "                    return image_urls\n",
    "\n",
    "\n",
    "            # move the result startpoint further down\n",
    "            results_start = len(thumbnail_results)\n",
    "\n",
    "        return image_urls\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    list_of_search_terms = [\n",
    "        \"people at weddings\",\n",
    "        \"depression human face\",\n",
    "        \"people at senate hearing\",\n",
    "        \"disgusted face\",\n",
    "        \"people shocked\"\n",
    "        ]\n",
    "\n",
    "    getFaces = FaceScraper()\n",
    "    getFaces.getImgUrls(list_of_search_terms, 200)\n",
    "    getFaces.extractFaces()\n",
    "    getFaces.saveCropped()\n",
    "\n",
    "# Run scrapper\n",
    "#run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This process increased or training data 3-fold. However, because the images were grabbed from a search engine, they also introduced some noise and bias.\n",
    "\n",
    "#### Recognition\n",
    "Our project's original aim was to deploy the model on a server hosted on a raspberry pi that gets video feeds from a client (headset). However, to meet the pi's hardware restrictions,\n",
    "we decided to run the first stage of our classifier on the client. The client then sends the cropped faces to the server over a TCP socket.\n",
    "\n",
    "*Code for client*\n",
    "\n",
    "***DO NOT RUN IN JUPYTER NOTEBOOK:*** The code uses openCV's imshow to display the lablled frame. This function is make the jupyter server crash."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "TCP_IP = '127.0.0.1'\n",
    "TCP_PORT = 9502\n",
    "video_file = 'facesVid.webm'\n",
    "\n",
    "def run_client():\n",
    "    # Receive facial expression labels from the server\n",
    "    def receiveLabels(mySocket):\n",
    "        data = mySocket.recv(1024)\n",
    "        print(str(data))\n",
    "        return str(data)\n",
    "\n",
    "    # detects the faces in a frame\n",
    "    def detectAndDisplay(frame):\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_gray = cv2.equalizeHist(frame_gray)\n",
    "        #-- Detect faces\n",
    "        faces = face_cascade.detectMultiScale(frame_gray)\n",
    "        face_list = []\n",
    "        i = 0\n",
    "        for (x,y,w,h) in faces:\n",
    "            center = (x + w//2, y + h//2)\n",
    "            face_list.append(frame[y:y+h, x:x+w])\n",
    "\n",
    "        return frame, face_list, faces\n",
    "\n",
    "    face_cascade_name = \"./face_detector.xml\"#args.face_cascade\n",
    "    face_cascade = cv2.CascadeClassifier()\n",
    "\n",
    "    #-- 1. Load the cascades\n",
    "    if not face_cascade.load(face_cascade_name):\n",
    "        print('--(!)Error loading face cascade')\n",
    "        exit(0)\n",
    "\n",
    "    print(\"Starting server...\\n\")\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # establishing a tcp connection\n",
    "    sock.bind((TCP_IP, TCP_PORT))\n",
    "    sock.listen(5)\n",
    "\n",
    "    while True:\n",
    "        (client_socket, client_address) = sock.accept()  # wait for server\n",
    "        print\n",
    "        'connection established with ' + str(client_address)\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        # send frames\n",
    "        while True:\n",
    "            flag, frame = cap.read()\n",
    "            labels = []\n",
    "            if flag:\n",
    "                frame_labelled, face_list, bounds = detectAndDisplay(frame)\n",
    "\n",
    "                for i in range(len(face_list)):\n",
    "                    a_face = face_list[i]\n",
    "                    a_face = pickle.dumps(a_face)\n",
    "                    size = len(a_face)\n",
    "                    p = struct.pack('I', size)\n",
    "                    a_face = p + a_face\n",
    "                    client_socket.sendall(a_face)\n",
    "\n",
    "                    label = receiveLabels(client_socket)\n",
    "\n",
    "                    frame = cv2.rectangle(\n",
    "                        frame,\n",
    "                        (bounds[i][0], bounds[i][1]),\n",
    "                        (bounds[i][0] + bounds[i][2], bounds[i][1] + bounds[i][3]),\n",
    "                        (0, 0, 0),\n",
    "                        1\n",
    "                    )\n",
    "                    frame = cv2.putText(frame, label, (bounds[i][0], bounds[i][1] + bounds[i][3] + 5), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "            else:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame - 1)\n",
    "\n",
    "            if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "                size = 10\n",
    "                p = struct.pack(\"I\", size)\n",
    "                client_socket.send(p)\n",
    "                client_socket.send('')\n",
    "                break\n",
    "\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "# Run client\n",
    "# run_client()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: The client uses server sockets. This is an unintended mistake. It will be fixed soon.*\n",
    "\n",
    "The server then uses the model trained on Google Colab to reply with a string\n",
    "containing the two most likely facial expressions.\n",
    "\n",
    "*Code for the server*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import keras\n",
    "import numpy\n",
    "import tensorflow\n",
    "import math\n",
    "\n",
    "def run_server():\n",
    "    print(\"Geting model files...\")\n",
    "    # # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    print(\"Loading models...\")\n",
    "    loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # #print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print(\"Model compiled\")\n",
    "\n",
    "    TCP_IP = '153.106.213.22'\n",
    "    TCP_PORT = 9502\n",
    "    server_address = (TCP_IP, TCP_PORT)\n",
    "    i = 0\n",
    "\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.connect((TCP_IP, TCP_PORT))\n",
    "    data = b''\n",
    "    payload_size = struct.calcsize(\"I\")\n",
    "\n",
    "    def getframe(data):\n",
    "        while len(data) < payload_size:\n",
    "            data += sock.recv(4096)\n",
    "        packed_msg_size = data[:payload_size]\n",
    "        data = data[payload_size:]\n",
    "        msg_size = struct.unpack(\"I\", packed_msg_size)[0]\n",
    "        while len(data) < msg_size:\n",
    "            data += sock.recv(4096)\n",
    "        frame_data = data[:msg_size]\n",
    "        data = data[msg_size:]\n",
    "\n",
    "        if frame_data == b'':\n",
    "            return -1, data, None\n",
    "\n",
    "        return 0, data, pickle.loads(frame_data)\n",
    "\n",
    "    # send feed\n",
    "    def sendLabel(socket_, prediction):\n",
    "        emotions = [\"neutral\", \"smiling\", \"sad\", \"surprise-shock\", \"angry\", \"disgusted\", \"fearful\"]\n",
    "\n",
    "        response = emotions[prediction.index(max(prediction))]\n",
    "        prediction.pop(prediction.index(max(prediction)))\n",
    "        if max(prediction) > 0.5:\n",
    "            response = response + \" / \" + emotions[prediction.index(max(prediction))]\n",
    "\n",
    "        socket_.send(bytearray(response, \"utf-8\"))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        flag, data, frame = getframe(data)\n",
    "        if (flag == -1):\n",
    "            break\n",
    "        frame = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        predictions = loaded_model.predict(numpy.reshape(frame, (1, 256, 256, 3)))\n",
    "        print(predictions)\n",
    "        sendLabel(sock, predictions[0].tolist())\n",
    "\n",
    "    sock.close()\n",
    "\n",
    "# Run server\n",
    "# run_server()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: The server uses client sockets. This is an unintended mistake. It will be fixed soon.*\n",
    "\n",
    "### Results & Implications\n",
    "\n",
    "### Conclusion\n",
    "Even if it was largely a failure, our project still demonstrates the versatility of facial recognition systems\n",
    "### Citations\n",
    "[TODO: Use proper citations]\n",
    "\n",
    "yolov3,\n",
    "title={YOLOv3: An Incremental Improvement},\n",
    "author={Redmon, Joseph and Farhadi, Ali},\n",
    "journal = {arXiv},\n",
    "year={2018}\n",
    "\n",
    "---\n",
    "\n",
    "https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}