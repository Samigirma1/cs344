{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Samuel Zeleke - CS344 - Homework 3\n",
    "\n",
    "1. Compute the following values using the restaurant example.\n",
    "\n",
    "    The **information gain** provided by using the price attribute as the root of the decision tree. Is it more or less \n",
    "valuable than the type or patrons attributes computed in class?\n",
    "\n",
    "    ANSWER:\n",
    "        Gain(Price) = 1 - Remainder(A)\n",
    "                    = 1 - ((3/12) * B(1/3) + (2/12) * B(2/2) + (7/12) * B(3/7))\n",
    "                    = 1 - (0.25 * -((1/3) * lg(1/3) + (2/3) * lg(2/3))\n",
    "                           + (1/6) * -((2/2) * lg(2/2) + (0/2) * lg(0/2)) # 0*lg(0) is undefined. But, since the right\n",
    "                                                                          # limit of x*lg(x) as x approaches 0 is 0, \n",
    "                                                                          # 0*lg(0) ~ 0\n",
    "                           + (7/12) * -((3/7) * lg(3/7) + (4/7) * lg(4/7))\n",
    "                      )\n",
    "                    = 1 - (0.25 * -(-0.52832 -0.389975) + \n",
    "                            (1/6) * -(0) +   \n",
    "                            (7/12) * -(-0.52388 - 0.46135) \n",
    "                          )\n",
    "                    = 1 - 0.80429\n",
    "                    = 0.1957\n",
    "                    \n",
    "    - Since the gain for `Patrons` is greater than the gain for `Price`. So, asking about `Price` is less valuable than \n",
    "    asking about `Patrons`.\n",
    "    - Since the gain for `Price` is greater than the gain for `Type`, it's more valuable than `Type`.\n",
    "\n",
    "2. In class, we attempted to create by hand a neural network that computes the XOR function. If this was possible, see \n",
    "   if you can simplify the network we built. Consider relaxing the conventions of densely-connected, sequential layers. \n",
    "   If it was not possible, give a full explanation why it canâ€™t be done.\n",
    "   \n",
    "   ANSWER:\n",
    "        The XOR function is not linearly separable. (There is no straight line that can segregate [(0, 0), (0, 1), (1, 0), (1, 1)] into [(0, 1), (1, 0)] and [(0, 0), (1, 1)].) So the function can not be represented by a single perceptron or Single-layered network. Instead, we should use two input nodes that learn to label ranges that include (0, 1) and/or (0,1). We should then use an output node that combines the two nodes ranges to only recognize both [(0, 1) and (1, 0)].\n",
    "        \n",
    "        Example \n",
    "            XOR(a, b) = AND(OR(a, b), NAND(a, b)); OR(a, b) recognizes [(0, 1), (1, 0), and (1, 1)], NAND(a, b) recognizes [(0, 1), (1, 0), and (0, 0)] and AND(...) intersects the two ranges.\n",
    "\n",
    "3. Use Python/NumPy/Pandas/Keras to load and manipulate the Boston Housing Dataset as follows.\n",
    "\n",
    "    a. Compute the dimensions of the data structures. Include code to print these values.\n",
    "    \n",
    "       ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "training data    dimension: 2, shape: 404 x 13\n",
      "training targets dimension: 1, shape: 404\n",
      "test data        dimension: 2, shape: 102 x 13\n",
      "test targets     dimension: 1, shape: 102\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\n",
    "\n",
    "print(\"training data    dimension: %d, shape: %d x %d\" %(train_data.ndim, train_data.shape[0], train_data.shape[1]))\n",
    "print(\"training targets dimension: %d, shape: %d\" %(train_targets.ndim, train_targets.shape[0]))\n",
    "print(\"test data        dimension: %d, shape: %d x %d\" %(test_data.ndim, test_data.shape[0], test_data.shape[1]))\n",
    "print(\"test targets     dimension: %d, shape: %d\" %(test_targets.ndim, test_targets.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3    \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    b. Construct a suitable testing set, training set, and validation set for this data. Submit code to create these \n",
    "       datasets but do not include the datasets themselves.\n",
    "       \n",
    "       ANSWER:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# The function partitions input training data into randomized training and validation\n",
    "# sets where the resulting training dataset is prop_of_training proportion of the original \n",
    "# data. \n",
    "# Returns: a dictionary containing the numpy arrays for the training, validation, and \n",
    "# testing datasets and their targets\n",
    "#\n",
    "def partition_data_to_numpy_arrays(\n",
    "        training, training_targets,\n",
    "        testing, testing_targets,\n",
    "        prop_of_training = 0.8):\n",
    "    \n",
    "    training_merged = numpy.hstack((training, training_targets.reshape(training_targets.shape[0], 1)))\n",
    "    numpy.random.shuffle(training_merged)\n",
    "    \n",
    "    testing_merged = numpy.hstack((testing, testing_targets.reshape(testing_targets.shape[0], 1)))\n",
    "    numpy.random.shuffle(testing_merged)\n",
    "\n",
    "    return {\n",
    "        \"training_data\" : training_merged[\n",
    "                          0:math.floor(prop_of_training * training_merged.shape[0]), \n",
    "                          0:training.shape[1] - 1\n",
    "                          ],\\\n",
    "           \"training_targets\" : training_merged[\n",
    "                          0:math.floor(prop_of_training * training_merged.shape[0]), \n",
    "                          training.shape[1] - 1\n",
    "                          ],\\\n",
    "           \"validation_data\" : training_merged[\n",
    "                          math.floor((1 - prop_of_training)*training_merged.shape[0]):, \n",
    "                          0:training.shape[1] - 1\n",
    "                          ],\\\n",
    "           \"validation_targets\" : training_merged[\n",
    "                          math.floor((1 - prop_of_training)*training_merged.shape[0]):, \n",
    "                          training.shape[1] - 1\n",
    "                          ],\\\n",
    "            \"testing_data\" : testing_merged[:,0:training.shape[1] - 1],\\\n",
    "            \"testing_targets\" : testing_merged[:,training.shape[1] - 1]\n",
    "    }\n",
    "\n",
    "# The function partitions input training data into randomizedm, labelled training and validation\n",
    "# sets where the resulting training dataset is prop_of_training proportion of the original \n",
    "# data. \n",
    "# Returns: a dictionary containing the dataframes for the training, validation, and \n",
    "# testing datasets and their targets\n",
    "#\n",
    "def partition_data_to_dataframe(\n",
    "        training, training_targets,\n",
    "        testing, testing_targets,\n",
    "        prop_of_training = 0.8):\n",
    "    \n",
    "    # labels retrieved from Statlib\n",
    "    # Variables in order:\n",
    "    # CRIM     per capita crime rate by town\n",
    "    # ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    # INDUS    proportion of non-retail business acres per town\n",
    "    # CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    # NOX      nitric oxides concentration (parts per 10 million)\n",
    "    # RM       average number of rooms per dwelling\n",
    "    # AGE      proportion of owner-occupied units built prior to 1940\n",
    "    # DIS      weighted distances to five Boston employment centres\n",
    "    # RAD      index of accessibility to radial highways\n",
    "    # TAX      full-value property-tax rate per $10,000\n",
    "    # PTRATIO  pupil-teacher ratio by town\n",
    "    # B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    # LSTAT    % lower status of the population\n",
    "    # MEDV     Median value of owner-occupied homes in $1000's\n",
    "    \n",
    "    labels = [\n",
    "        \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \n",
    "        \"NOX\", \"RM\", \"AGE\", \"DIS\", \n",
    "        \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \n",
    "        \"LSTAT\",\"MEDV\"\n",
    "    ]\n",
    "    \n",
    "    training_dataframe = pandas.DataFrame()\n",
    "    testing_dataframe = pandas.DataFrame()\n",
    "    \n",
    "    for i in range(len(labels) - 1):\n",
    "        training_dataframe[labels[i]] = pandas.Series(training[:, i])\n",
    "        testing_dataframe[labels[i]] = pandas.Series(testing[:, i])\n",
    "    \n",
    "    training_dataframe[\"MEDV\"] = pandas.Series(training_targets)\n",
    "    testing_dataframe[\"MEDV\"] = pandas.Series(testing_targets)\n",
    "    \n",
    "    training_dataframe = training_dataframe.reindex(\n",
    "        numpy.random.permutation(training_dataframe.index)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "            \"training_data\" : training_dataframe.head(\n",
    "                math.floor(training_dataframe.shape[0] * (prop_of_training)))[labels[:len(labels) - 1]],\\\n",
    "           \"training_targets\" : pandas.DataFrame(training_dataframe.head(\n",
    "                math.floor(training_dataframe.shape[0] * (prop_of_training)))[labels[len(labels) - 1]]),\\\n",
    "           \"validation_data\" : training_dataframe.tail(\n",
    "                math.floor(training_dataframe.shape[0] * (1 - prop_of_training)))[labels[:len(labels) - 1]],\\\n",
    "           \"validation_targets\" : pandas.DataFrame(training_dataframe.tail(\n",
    "                math.floor(training_dataframe.shape[0] * (1 - prop_of_training)))[labels[len(labels) - 1]]),\\\n",
    "            \"testing_data\" : testing_dataframe[labels[:len(labels) - 1]],\\\n",
    "            \"testing_targets\" : pandas.DataFrame(testing_dataframe[labels[len(labels) - 1]])\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# x = partition_data_to_numpy_arrays(\n",
    "#     training = train_data, \n",
    "#     training_targets = train_targets,\n",
    "#     testing = test_data, \n",
    "#     testing_targets = test_targets)\n",
    "# y = partition_data_to_dataframe(\n",
    "#     training = train_data, \n",
    "#     training_targets = train_targets,\n",
    "#     testing = test_data, \n",
    "#     testing_targets = test_targets)\n",
    "# print(x)\n",
    "# y[\"training_data\"].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    c. Create one new synthetic feature that could be useful for machine learning in this domain. Explain what it is and\n",
    "       why it might be useful, and submit code to add it to the dataset.\n",
    "       \n",
    "       ANSWER:\n",
    "            Synthetic Feature: \n",
    "                \n",
    "                residentiality = ZN : INDUS; \n",
    "                \n",
    "                The feature mesures how \"residential\" a town is. A higher ratio indicates a town where more land was zoned for residential purposes than for businesses. Houses in quieter, more residential areas (high residentiality ratio) should have a higher value.\n",
    "                \n",
    "                However, the two features are fairly negatively correlated. So, the synthetic feature may have an eaggerated influence on the target. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "       ZN  INDUS  RESIDENTIALITY\n369   0.0  18.10        0.000000\n61   20.0   6.96        2.873563\n384   0.0   4.39        0.000000\n277  22.0   5.86        3.754266\n113   0.0   9.69        0.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>RESIDENTIALITY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>369</th>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>20.0</td>\n      <td>6.96</td>\n      <td>2.873563</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>0.0</td>\n      <td>4.39</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>22.0</td>\n      <td>5.86</td>\n      <td>3.754266</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 116
    }
   ],
   "source": [
    "partitioned_randomized_data = partition_data_to_dataframe(\n",
    "    training = train_data, \n",
    "    training_targets = train_targets,\n",
    "    testing = test_data, \n",
    "    testing_targets = test_targets\n",
    ")\n",
    "# Add the synthetic feature the the datasets\n",
    "for datasets in partitioned_randomized_data:\n",
    "    # check if the current dataset is a dataframe of examples by using the number of columns\n",
    "    if partitioned_randomized_data[datasets].shape[1] == 13:\n",
    "        partitioned_randomized_data[datasets][\"RESIDENTIALITY\"] = \\\n",
    "            partitioned_randomized_data[datasets][\"ZN\"] / partitioned_randomized_data[datasets][\"INDUS\"]\n",
    "partitioned_randomized_data[\"training_data\"][[\"ZN\", \"INDUS\", \"RESIDENTIALITY\"]].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-eb739bdf",
   "language": "python",
   "display_name": "PyCharm (CS344)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}